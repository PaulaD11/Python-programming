# -*- coding: utf-8 -*-
"""Regresión_logística.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hg78b-3LI8UjQOmp8Jl-7sT5alFKfYvX
"""

#Leemos las librerías necesarias
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
import seaborn as sns

from patsy import dmatrices
from scipy import stats
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

#Se carga la base de datos Affairs
Datos=sm.datasets.fair.load_pandas().data

#Se calcula la correlación entre las variables
sns.heatmap(Datos.corr(), annot=True)

#Creamos la dummy de affairs en la columna Infidelity
Datos['infidelity'] = (Datos.affairs > 0).astype(int)

#Definimos las variables X y Y
X = Datos.drop(columns = ['infidelity', 'affairs'])
y = Datos['infidelity']

#Separamos la base de datos entre los datos de entrenamiento y los datos de prueba
#75% de los datos para entrenar el modelo
#25% restante para evaluarlo
X_train, X_test, y_train, y_test, = train_test_split(X,y, test_size=0.25, random_state=0)

#Corremos la regresión con los datos de entrenamiento
LogReg=LogisticRegression()
mdl=LogReg.fit(X_train,y_train)

#El modelo tiene una precisión del 72%
mdl.score(X_train, y_train)

#Obtenemos los coeficientes para las variables
pd.DataFrame(list(zip(X.columns, np.transpose(mdl.coef_))))

# Predicciones con los datos de prueba
predicted = mdl.predict(X_test)
print(predicted)

#Predicciones con datos aleatorios
mdl.predict(np.array([[5, 38, 15, 3, 3, 12, 2, 2 ]]))

#Matriz de confusión
cnf_matrix = confusion_matrix(y_test, predicted)
cnf_matrix
#Verdaderos positivos: Estos son los casos en los que el modelo predijo correctamente la clase positiva. TP 988
#Verdaderos negativos: Estos son los casos en los que el modelo predijo correctamente la clase negativa. TN 178
#Falsos positivos: Estos son los casos en los que el modelo predijo incorrectamente la clase positiva cuando la verdadera clase es negativa. FP 314
#Falsos negativos:Estos son los casos en los que el modelo predijo incorrectamente la clase negativa cuando la verdadera clase es positiva. FN 112

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score
#Proporción de predicciones correctas, en comparación con el total de predicciones realizadas
print("Accuracy:", metrics.accuracy_score(y_test, predicted))
#Proporción de predicciones positivas correctas, en comparación con el total de predicciones positivas
print("Precision:", metrics.precision_score(y_test, predicted))

report = classification_report(y_test, predicted)
roc_auc = roc_auc_score(y_test, predicted)
print("Classification Report:\n", report)
print("ROC AUC:", roc_auc)

#Precisión (Precision): Proporción de predicciones positivas correctas en relación con todas las predicciones positivas

#Recuperación (Recall): Proporción de predicciones positivas correctas en relación con el total de postivos reales (TP/TP+FN)

#Puntuación F1 (F1 Score): Media armónica ponderada de precisión y recuperación. Cuanto más se acerque a 1 mejor será el modelo

#Soporte (Support): El número real de instancias en cada clase en el conjunto de prueba.

#El Área bajo la Curva ROC (AUC-ROC) es una medida numérica de la capacidad de discriminación del modelo.
#Cuanto mayor sea el AUC-ROC, mejor será el rendimiento del modelo para distinguir entre las clases.
#El AUC-ROC puede tener valores entre 0 y 1.
#Un modelo con AUC-ROC de 0.5 tiene un rendimiento similar al azar, mientras que un modelo con AUC-ROC de 1.0 tiene un rendimiento perfecto.