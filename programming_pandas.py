# -*- coding: utf-8 -*-
"""Programming Pandas

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PY2wqkEJ1B_6HccAwgBaZ23o5Tnnzq2J
"""

import pandas as pd

Data=pd.read_csv("/content/socialmedia.csv")

#Consultar los primeros 10 registros de la data
Data.head(10)

#Consultar los últimos 10 registros de la data
Data.tail(10)

#Consultar la forma del dataset (Filas, Columnas)
Data.shape

#Consultar los nombres de las columnas de la data
Data.columns.values

#Consultar las principales medidas estadísticas del dataset
Data.describe()

#Consultar qué tipo de dato tiene cada variable
Data.dtypes

#Filtro los registros de Instagram
Instagram=Data[Data['Platform']=="Instagram"]
Instagram.head()

#Filtro Likes/Reactions mayores a 1200
mv_likes=Data[Data['Likes/Reactions']>1200]
mv_likes

#Filtro Likes/Reactions menores a 500
menor_likes=Data[Data['Likes/Reactions']<500]
menor_likes

#Filtro comments entre 120 y 190 con and
comments=Data[(Data['Comments']>120) & (Data['Comments']<190)]
#Filtro comments entre 120 y 190 con or

comments_2=Data[(Data['Comments']>120) | (Data['Comments']<190)]

#Filtro los registros que no son Twitter
Isnot=Data[Data['Platform']!="Twitter"]
Isnot

#Selecciono solo algunas columnas de la data
short_data=Data[['Platform','Comments']]
short_data

#Agregar una nueva columna a la data
#df['new'] = df['W'] + df['Y']
#Eliminar una columna de la data
#df.drop('new',axis='columns')

#Buscar los valores null por columna
Data.isna().any()
#Conteo de los valores nulo
Data.isna().sum()

#Suma de la columna comments
Data['Comments'].sum()

#Cálculos con las columnas de la data y crear una nueva columna
Data['Cálculos']=(2*Data['Comments']/10)+Data['Likes/Reactions']

#Eliminar una columna
Data.drop('Media Type',axis='columns')

#eliminar filas con valores nulos
Data.dropna(axis=0, how="any")

#Rellenar na de una columna con el promedio de los demás valores
col=Data['Comments'].fillna(Data['Comments'].mean())

#Seleccionar filas por su posición
Data.loc[[3,5]]

#Generar variables dummy
dummy_mediatype=pd.get_dummies(Data['Media Type'], prefix = "Archivo")
print(dummy_mediatype)

#Genero 2 dataframes para luego concatenarlos
df1=Data.loc[[0,9]]
df2=Data.loc[[10,20]]
concat=pd.concat([df1,df2])
concat

#Ordenar la data por una columna Shares/Retweets de menor a mayor
Data.sort_values("Shares/Retweets")

#ordenar la data por una columna Shares/Retweets de mayor a menor
Data.sort_values("Shares/Retweets", ascending=False)

#Cambiar nombre a una columna
Data.rename(columns={'Likes/Reactions':'Likes'})

#Eliminar columnas
Data.drop(columns='Media Type')

#Eliminar filas duplicadas basado en todas las columnas por default
Data.drop_duplicates()
#Eliminar duplicados con base en una columna específica
Data.drop_duplicates(subset=['Platform'])

#seleccción aleatoria de filas
Data.sample(n=3)

#seleccionar filas de la 10 a la 20D
Data_2.iloc[:, [0,2,1]]

#Hallar el top n más grande
Data.nlargest(5,'Comments')

#Seleccionar el topn más pequeño
Data.nsmallest(5,'Likes/Reactions')

#Seleccionar columnas
Data[["Likes/Reactions", "Comments", "Platform"]]

#Aquí seleccionamos a las columnas que inicen su nombre con "Accepted".
#df.filter(regex = '^Accepted')

#Aquí seleccionamos a las columnas que terminen su nombre con "home".
#df.filter(regex = 'home$')

#Aquí seleccionamos las columnas desde "Year_Birth" hasta "Kidhome" incluyéndose.
#df.loc[:,'Year_Birth':'Kidhome']

#Aquí se incluyen las columnas 1, 2 y 5.
#df.iloc[:,[1,2,5]]

#Encontrar la cantidad de valores en cada categoría de la columna
Data['Platform'].value_counts()

#Encontrar el número de filas del dataframe
len(Data)

#Cantidad de valores distintos en la columna
Data['Account Verification'].nunique()

#Hallar las principales medidas estadísticas
Data.sum()
Data.count()
Data.median()
Data.min()
Data.max()
Data.mean()
Data.mode()
Data.var()
Data.std()
Data.quantile([0.25])
#Covarianza
#s1.cov(s2)
#Covarianza con Numpy
#np.cov(s1,s2)

#Hacemos joins a las tablas

#Primero establecemos la columna llave para las dos tablas
#df1=df1.set_index("name")
#df2=df2.set_index("name")

#Outer join
#pd.merge(df1,df2,how="outer",left_index=True,right_index=True)

#inner join
#pd.merge(df1,df2,how="inner",left_index=True,right_index=True)

#Left join
#pd.merge(df1,df2,how="left",left_index=True,right_index=True)

#Right join
#pd.merge(df1,df2,how="right",left_index=True,right_index=True)

#Otra forma de hacer joins de cualquier tipo
#pd.merge(df1,df2,how="right",on="name")

#Unir columnas en modo de filas
#pd.melt(df1)

#Pasar filas a modo de columnas
#Data.pivot(columns="var", values="val")

#Unir filas Dataframes
#pd.concat([df1,df2])

#Unir columnas dataframes
#pd.concat([df1,df2], axis=1)

#Agrupar por un criterio las filas

Groupby=Data.groupby("Platform")

#Ahora puedo, por ejemplo, obtener el promedio por cada categoría de la columna
Groupby.mean()

#Detectar si hay datos faltantes
Data.isna()

#Detectar si hay datos faltantes por columna
Data.isna().any()

#Conteo de datos nulos por columna
Data.isna().sum()

#Eliminar valores nulos por fila
Data.dropna()

#Eliminar valores nulos por columna
Data.dropna(axis=1)

#Eliminar filas que tengan 2 o más valores nulos
Data.dropna(thresh=2)

#Valores unicos
Data['Media Type'].unique()

#Conteo de valores unicos
Data['Platform'].nunique()

#Conteo de las veces que se repite cada valor
Data['Platform'].value_counts()

#Valores duplicados
Data.duplicated()

#Conteo de duplicados
Data.duplicated().sum()

#Eliminar duplicados
Data.drop_duplicates()

#También se puede por columna
Data.duplicated(['Platform'])